.. _loading_data:

============
æ•°æ®åŠ è½½
============

Ray Data ä»å„ç§æ¥æºåŠ è½½æ•°æ®ã€‚æœ¬æŒ‡å—å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

* `è¯»å–å›¾åƒ <#reading-files>`_ ç­‰æ–‡ä»¶
* `åŠ è½½å†…å­˜æ•°æ® <#loading-data-from-other-libraries>`_ å¦‚ pandas DataFrames
* `è¯»å–æ•°æ® <#reading-databases>`_ å¦‚ MySQL

.. _reading-files:

è¯»å–æ–‡ä»¶
=============

Ray Data ä»æœ¬åœ°ç£ç›˜æˆ–äº‘å­˜å‚¨ä¸­è¯»å–å¤šç§æ–‡ä»¶æ ¼å¼çš„æ–‡ä»¶ã€‚
è¦æŸ¥çœ‹æ”¯æŒçš„æ–‡ä»¶æ ¼å¼çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·å‚é˜…
:ref:`Input/Output å‚è€ƒ <input-output>`ã€‚

.. tab-set::

    .. tab-item:: Parquet

        è¦è¯»å– Parquet æ–‡ä»¶ï¼Œè¯·è°ƒç”¨ :func:`~ray.data.read_parquet`ã€‚

        .. testcode::
            :skipif: True

            import ray

            ds = ray.data.read_parquet("local:///tmp/iris.parquet")

            print(ds.schema())

        .. testoutput::

            Column        Type
            ------        ----
            sepal.length  double
            sepal.width   double
            petal.length  double
            petal.width   double
            variety       string

    .. tab-item:: Images

        è¯»å–åŸå§‹å›¾åƒï¼Œè¯·è°ƒç”¨ :func:`~ray.data.read_images`ã€‚Ray Data å°†å›¾åƒè¡¨ç¤ºä¸º NumPy ndarrayã€‚

        .. testcode::
            :skipif: True

            import ray

            ds = ray.data.read_images("local:///tmp/batoidea/JPEGImages/")

            print(ds.schema())

        .. testoutput::
            :skipif: True

            Column  Type
            ------  ----
            image   numpy.ndarray(shape=(32, 32, 3), dtype=uint8)

    .. tab-item:: Text

        è¦è¯»å–æ–‡æœ¬è¡Œï¼Œè¯·è°ƒç”¨ :func:`~ray.data.read_text`ã€‚

        .. testcode::
            :skipif: True

            import ray

            ds = ray.data.read_text("local:///tmp/this.txt")

            print(ds.schema())

        .. testoutput::

            Column  Type
            ------  ----
            text    string

    .. tab-item:: CSV

        è¦è¯»å– CSV æ–‡ä»¶ï¼Œè¯·è°ƒç”¨ :func:`~ray.data.read_csv`ã€‚

        .. testcode::
            :skipif: True

            import ray

            ds = ray.data.read_csv("local:///tmp/iris.csv")

            print(ds.schema())

        .. testoutput::

            Column             Type
            ------             ----
            sepal length (cm)  double
            sepal width (cm)   double
            petal length (cm)  double
            petal width (cm)   double
            target             int64

    .. tab-item:: Binary

        è¦è¯»å–åŸå§‹äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œè¯·è°ƒç”¨ :func:`~ray.data.read_binary_files`ã€‚

        .. testcode::
            :skipif: True

            import ray

            ds = ray.data.read_binary_files("local:///tmp/file.dat")

            print(ds.schema())

        .. testoutput::

            Column  Type
            ------  ----
            bytes   binary

    .. tab-item:: TFRecords

        è¦è¯»å– TFRecords æ–‡ä»¶ï¼Œè¯·è°ƒç”¨ :func:`~ray.data.read_tfrecords`ã€‚

        .. testcode::
            :skipif: True

            import ray

            ds = ray.data.read_tfrecords("local:///tmp/iris.tfrecords")

            print(ds.schema())

        .. testoutput::

            Column             Type
            ------             ----
            sepal length (cm)  double
            sepal width (cm)   double
            petal length (cm)  double
            petal width (cm)   double
            target             int64

ä»æœ¬åœ°ç£ç›˜è¯»å–æ–‡ä»¶
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

è¦ä»æœ¬åœ°ç£ç›˜è¯»å–æ–‡ä»¶ï¼Œè¯·è°ƒç”¨å¦‚ :func:`~ray.data.read_parquet` å‡½æ•°ï¼Œå¹¶ä½¿ç”¨
``local://`` åè®®æŒ‡å®šè·¯å¾„ã€‚è·¯å¾„å¯ä»¥æŒ‡å‘æ–‡ä»¶æˆ–ç›®å½•ã€‚

è¦è¯»å– Parquet ä»¥å¤–çš„æ ¼å¼ï¼Œè¯·å‚é˜… :ref:`Input/Output å‚è€ƒ <input-output>`ã€‚

.. tip::

    å¦‚æœæ‚¨çš„æ–‡ä»¶å¯ä»¥åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè®¿é—®ï¼Œè¯·æ’é™¤ ``local://`` ä»¥åœ¨é›†ç¾¤ä¸­å¹¶è¡Œè¯»å–ä»»åŠ¡ã€‚

.. testcode::
    :skipif: True

    import ray

    ds = ray.data.read_parquet("local:///tmp/iris.parquet")

    print(ds.schema())

.. testoutput::

    Column        Type
    ------        ----
    sepal.length  double
    sepal.width   double
    petal.length  double
    petal.width   double
    variety       string

ä»äº‘å­˜å‚¨è¯»å–æ–‡ä»¶
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

è¦è¯»å–äº‘å­˜å‚¨ä¸­çš„æ–‡ä»¶ï¼Œè¯·å‘äº‘æœåŠ¡æä¾›å•†éªŒè¯æ‰€æœ‰èŠ‚ç‚¹ã€‚ç„¶åï¼Œè°ƒç”¨ç±»ä¼¼æ–¹æ³•
:func:`~ray.data.read_parquet` å¹¶æŒ‡å®šå…·æœ‰é€‚å½“æ¶æ„çš„ URIã€‚ 
URI å¯ä»¥æŒ‡å‘å­˜å‚¨æ¡¶ã€æ–‡ä»¶å¤¹æˆ–å¯¹è±¡ã€‚

è¦è¯»å– Parquet ä»¥å¤–çš„æ ¼å¼ï¼Œè¯·å‚é˜… :ref:`Input/Output å‚è€ƒ <input-output>`ã€‚

.. tab-set::

    .. tab-item:: S3

        è¦ä» Amazon S3 è¯»å–æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ ``s3://`` åè®®ã€‚

        .. testcode::

            import ray

            ds = ray.data.read_parquet("s3://anonymous@ray-example-data/iris.parquet")

            print(ds.schema())

        .. testoutput::

            Column        Type
            ------        ----
            sepal.length  double
            sepal.width   double
            petal.length  double
            petal.width   double
            variety       string

    .. tab-item:: GCS

        è¦ä» Google Cloud Storage è¯»å–æ–‡ä»¶ï¼Œè¯·å®‰è£…
        `Google Cloud Storage çš„æ–‡ä»¶ç³»ç»Ÿæ¥å£ <https://gcsfs.readthedocs.io/en/latest/>`_

        .. code-block:: console

            pip install gcsfs

        ç„¶åï¼Œåˆ›å»ºä¸€ä¸ª ``GCSFileSystem`` å¹¶ä½¿ç”¨ ``gcs://`` æŒ‡å®š URIã€‚

        .. testcode::
            :skipif: True

            import ray

            ds = ray.data.read_parquet("s3://anonymous@ray-example-data/iris.parquet")

            print(ds.schema())

        .. testoutput::

            Column        Type
            ------        ----
            sepal.length  double
            sepal.width   double
            petal.length  double
            petal.width   double
            variety       string

    .. tab-item:: ABL

        è¦ä» Azure Blob å­˜å‚¨è¯»å–æ–‡ä»¶ï¼Œè¯·å°†
        `æ–‡ä»¶ç³»ç»Ÿæ¥å£å®‰è£…åˆ° Azure-Datalake Gen1 å’Œ Gen2 å­˜å‚¨ <https://pypi.org/project/adlfs/>`_

        .. code-block:: console

            pip install adlfs

        ç„¶åï¼Œåˆ›å»ºä¸€ä¸ª ``AzureBlobFileSystem`` å¹¶ä½¿ç”¨ `az://` åè®®çš„ URIã€‚

        .. testcode::
            :skipif: True

            import adlfs
            import ray

            ds = ray.data.read_parquet(
                "az://ray-example-data/iris.parquet",
                adlfs.AzureBlobFileSystem(account_name="azureopendatastorage")
            )

            print(ds.schema())

        .. testoutput::

            Column        Type
            ------        ----
            sepal.length  double
            sepal.width   double
            petal.length  double
            petal.width   double
            variety       string

ä» NFS è¯»å–æ–‡ä»¶
~~~~~~~~~~~~~~~~~~~~~~

è¦ä» NFS æ–‡ä»¶ç³»ç»Ÿè¯»å–æ–‡ä»¶ï¼Œè¯·è°ƒç”¨ç±»ä¼¼å‡½æ•° :func:`~ray.data.read_parquet`
å¹¶æŒ‡å®šå·²æŒ‚è½½æ–‡ä»¶ç³»ç»Ÿä¸Šçš„æ–‡ä»¶ã€‚è·¯å¾„å¯ä»¥æŒ‡å‘æ–‡ä»¶æˆ–ç›®å½•ã€‚

è¦è¯»å– Parquet ä»¥å¤–çš„æ ¼å¼ï¼Œè¯·å‚é˜… :ref:`Input/Output å‚è€ƒ <input-output>`ã€‚

.. testcode::
    :skipif: True

    import ray

    ds = ray.data.read_parquet("/mnt/cluster_storage/iris.parquet")

    print(ds.schema())

.. testoutput::

    Column        Type
    ------        ----
    sepal.length  double
    sepal.width   double
    petal.length  double
    petal.width   double
    variety       string

å¤„ç†å‹ç¼©æ–‡ä»¶
~~~~~~~~~~~~~~~~~~~~~~~~~

è¦è¯»å–å‹ç¼©æ–‡ä»¶ï¼Œè¯·å† ``compression`` ä¸­æŒ‡å®š ``arrow_open_stream_args`` ã€‚
æ‚¨å¯ä»¥ä½¿ç”¨ `Arrow æ”¯æŒçš„ä»»ä½•ç¼–è§£ç å™¨ <https://arrow.apache.org/docs/python/generated/pyarrow.CompressedInputStream.html>`__ã€‚

.. testcode::

    import ray

    ds = ray.data.read_csv(
        "s3://anonymous@ray-example-data/iris.csv.gz",
        arrow_open_stream_args={"compression": "gzip"},
    )

ä»å…¶ä»–åº“åŠ è½½æ•°æ®
=================================

ä»å•èŠ‚ç‚¹æ•°æ®åº“åŠ è½½æ•°æ®
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ray Data ä¸ pandasã€NumPy å’Œ Arrow ç­‰åº“è¿›è¡Œäº’æ“ä½œã€‚

.. tab-set::

    .. tab-item:: Python objects

        è¦ä»Python å¯¹è±¡åˆ›å»º :class:`~ray.data.dataset.Dataset` ï¼Œè°ƒç”¨
        :func:`~ray.data.from_items` å¹¶ä¼ å…¥ ``Dict``. Ray Data å°†æ¯ä¸ªæ® ``Dict`` æ•°æ®è§†ä¸ºä¸€è¡Œã€‚

        .. testcode::

            import ray

            ds = ray.data.from_items([
                {"food": "spam", "price": 9.34},
                {"food": "ham", "price": 5.37},
                {"food": "eggs", "price": 0.94}
            ])

            print(ds)

        .. testoutput::

            MaterializedDataset(
               num_blocks=3,
               num_rows=3,
               schema={food: string, price: double}
            )

        æ‚¨è¿˜å¯ä»¥ä»å¸¸è§„ Python å¯¹è±¡åˆ—è¡¨ä¸­åˆ›å»ºä¸€ä¸ª :class:`~ray.data.dataset.Dataset` ã€‚

        .. testcode::

            import ray

            ds = ray.data.from_items([1, 2, 3, 4, 5])

            print(ds)

        .. testoutput::

            MaterializedDataset(num_blocks=5, num_rows=5, schema={item: int64})

    .. tab-item:: NumPy

        To create a :class:`~ray.data.dataset.Dataset` from a NumPy array, call
        :func:`~ray.data.from_numpy`. Ray Data treats the outer axis as the row
        dimension.

        .. testcode::

            import numpy as np
            import ray

            array = np.ones((3, 2, 2))
            ds = ray.data.from_numpy(array)

            print(ds)

        .. testoutput::

            MaterializedDataset(
               num_blocks=1,
               num_rows=3,
               schema={data: numpy.ndarray(shape=(2, 2), dtype=double)}
            )

    .. tab-item:: pandas

        To create a :class:`~ray.data.dataset.Dataset` from a pandas DataFrame, call
        :func:`~ray.data.from_pandas`.

        .. testcode::

            import pandas as pd
            import ray

            df = pd.DataFrame({
                "food": ["spam", "ham", "eggs"],
                "price": [9.34, 5.37, 0.94]
            })
            ds = ray.data.from_pandas(df)

            print(ds)

        .. testoutput::

            MaterializedDataset(
               num_blocks=1,
               num_rows=3,
               schema={food: object, price: float64}
            )

    .. tab-item:: PyArrow

        To create a :class:`~ray.data.dataset.Dataset` from an Arrow table, call
        :func:`~ray.data.from_arrow`.

        .. testcode::

            import pyarrow as pa

            table = pa.table({
                "food": ["spam", "ham", "eggs"],
                "price": [9.34, 5.37, 0.94]
            })
            ds = ray.data.from_arrow(table)

            print(ds)

        .. testoutput::

            MaterializedDataset(
               num_blocks=1,
               num_rows=3,
               schema={food: string, price: double}
            )

.. _loading_datasets_from_distributed_df:

ä»åˆ†å¸ƒå¼ DataFrame åº“åŠ è½½æ•°æ®
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ray Data ä¸
:ref:`Dask <dask-on-ray>`ã€ :ref:`Spark <spark-on-ray>`ã€ :ref:`Modin <modin-on-ray>` å’Œ
:ref:`Mars <mars-on-ray>` ç­‰åˆ†å¸ƒå¼æ•°æ®å¤„ç†æ¡†æ¶è¿›è¡Œäº’æ“ä½œ ã€‚

.. tab-set::

    .. tab-item:: Dask

        è¦ä»
        `Dask DataFrame <https://docs.dask.org/en/stable/dataframe.html>`__ åˆ›å»º :class:`~ray.data.dataset.Dataset`ï¼Œè°ƒç”¨
        :func:`~ray.data.from_dask`ã€‚
        è¯¥å‡½æ•°æ„é€ ä¸€ä¸ªç”± Dask DataFrame çš„åˆ†å¸ƒå¼ Pandas DataFrame åˆ†åŒºæ”¯æŒçš„ ``Dataset``ã€‚

        .. testcode::
            :skipif: True

            import dask.dataframe as dd
            import pandas as pd
            import ray

            df = pd.DataFrame({"col1": list(range(10000)), "col2": list(map(str, range(10000)))})
            ddf = dd.from_pandas(df, npartitions=4)
            # Create a Dataset from a Dask DataFrame.
            ds = ray.data.from_dask(ddf)

            ds.show(3)

        .. testoutput::

            {'string': 'spam', 'number': 0}
            {'string': 'ham', 'number': 1}
            {'string': 'eggs', 'number': 2}

    .. tab-item:: Spark

        ä» `Spark DataFrame
        <https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html>`__ åˆ›å»º :class:`~ray.data.dataset.Dataset`ï¼Œè°ƒç”¨
        :func:`~ray.data.from_spark`. 
        è¯¥å‡½æ•°æ„é€ ä¸€ä¸ªç”± Spark DataFrame çš„åˆ†å¸ƒå¼ Pandas DataFrame åˆ†åŒºæ”¯æŒçš„ ``Dataset``ã€‚

        .. testcode::
            :skipif: True

            import ray
            import raydp

            spark = raydp.init_spark(app_name="Spark -> Datasets Example",
                                    num_executors=2,
                                    executor_cores=2,
                                    executor_memory="500MB")
            df = spark.createDataFrame([(i, str(i)) for i in range(10000)], ["col1", "col2"])
            ds = ray.data.from_spark(df)

            ds.show(3)

        .. testoutput::

            {'col1': 0, 'col2': '0'}
            {'col1': 1, 'col2': '1'}
            {'col1': 2, 'col2': '2'}

    .. tab-item:: Modin

        è¦ä» Modin DataFrame åˆ›å»º :class:`~ray.data.dataset.Dataset`ï¼Œè°ƒç”¨
        :func:`~ray.data.from_modin`ã€‚
        è¯¥å‡½æ•°æ„é€ ä¸€ä¸ªç”± Modin DataFrame çš„åˆ†å¸ƒå¼ Pandas DataFrame åˆ†åŒºæ”¯æŒçš„ ``Dataset``ã€‚

        .. testcode::
            :skipif: True

            import modin.pandas as md
            import pandas as pd
            import ray

            df = pd.DataFrame({"col1": list(range(10000)), "col2": list(map(str, range(10000)))})
            mdf = md.DataFrame(df)
            # Create a Dataset from a Modin DataFrame.
            ds = ray.data.from_modin(mdf)

            ds.show(3)

        .. testoutput::

            {'col1': 0, 'col2': '0'}
            {'col1': 1, 'col2': '1'}
            {'col1': 2, 'col2': '2'}

    .. tab-item:: Mars

        ä»  Mars DataFrame åˆ›å»º :class:`~ray.data.dataset.Dataset` ï¼Œè°ƒç”¨
        :func:`~ray.data.from_mars`. 
        è¯¥å‡½æ•°æ„é€ ä¸€ä¸ªç”± Mars DataFrame çš„åˆ†å¸ƒå¼ Pandas DataFrame åˆ†åŒºæ”¯æŒçš„ ``Dataset``ã€‚

        .. testcode::
            :skipif: True

            import mars
            import mars.dataframe as md
            import pandas as pd
            import ray

            cluster = mars.new_cluster_in_ray(worker_num=2, worker_cpu=1)

            df = pd.DataFrame({"col1": list(range(10000)), "col2": list(map(str, range(10000)))})
            mdf = md.DataFrame(df, num_partitions=8)
            # Create a tabular Dataset from a Mars DataFrame.
            ds = ray.data.from_mars(mdf)

            ds.show(3)

        .. testoutput::

            {'col1': 0, 'col2': '0'}
            {'col1': 1, 'col2': '1'}
            {'col1': 2, 'col2': '2'}

.. _loading_datasets_from_ml_libraries:

ä» ML åº“åŠ è½½æ•°æ®
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ray Data ä¸ HuggingFace å’Œ TensorFlow æ•°æ®é›†äº’æ“ä½œã€‚

.. tab-set::

    .. tab-item:: HuggingFace

        è¦å°† ğŸ¤— æ•°æ®é›†è½¬æ¢ä¸º Ray æ•°æ®é›†ï¼Œè¯·è°ƒç”¨
        :func:`~ray.data.from_huggingface`ã€‚
        è¯¥å‡½æ•°æ„é€ ä¸€ä¸ªç”± ğŸ¤— æ•°æ®é›†çš„åˆ†å¸ƒå¼ Pandas DataFrame åˆ†åŒºæ”¯æŒçš„ ``Dataset``ã€‚

        .. warning::
            :class:`~ray.data.from_huggingface` ä¸æ”¯æŒå¹¶è¡Œè¯»å–ã€‚å¯¹äºå†…å­˜ä¸­ ğŸ¤— æ•°æ®é›†æ¥è¯´è¿™ä¸æ˜¯é—®é¢˜ï¼Œä½†å¯¹äºå¤§å‹å†…å­˜æ˜ å°„ ğŸ¤— æ•°æ®é›†å¯èƒ½ä¼šå¤±è´¥ã€‚æ­¤å¤–ï¼Œ ğŸ¤— ``IterableDataset`` å¯¹è±¡ä¸æ”¯æŒã€‚

        .. testcode::

            import ray.data
            from datasets import load_dataset

            hf_ds = load_dataset("wikitext", "wikitext-2-raw-v1")
            ray_ds = ray.data.from_huggingface(hf_ds["train"])
            ray_ds.take(2)

        .. testoutput::
            :options: +MOCK

            [{'text': ''}, {'text': ' = Valkyria Chronicles III = \n'}]

    .. tab-item:: TensorFlow

        è¦è½¬æ¢ TensorFlow dataset ä¸º Ray Datasetï¼Œè°ƒç”¨ :func:`~ray.data.from_tf`ã€‚

        .. warning::
            :class:`~ray.data.from_tf` ä¸æ”¯æŒå¹¶è¡Œè¯»å–ã€‚ä»…å°†æ­¤å‡½æ•°ç”¨äº MNIST æˆ– CIFAR ç­‰å°å‹æ•°æ®é›†ã€‚

        .. testcode::

            import ray
            import tensorflow_datasets as tfds

            tf_ds, _ = tfds.load("cifar10", split=["train", "test"])
            ds = ray.data.from_tf(tf_ds)

            print(ds)

        ..
            The following `testoutput` is mocked to avoid illustrating download logs like
            "Downloading and preparing dataset 162.17 MiB".

        .. testoutput::
            :options: +MOCK

            MaterializedDataset(
               num_blocks=...,
               num_rows=50000,
               schema={
                  id: binary,
                  image: numpy.ndarray(shape=(32, 32, 3), dtype=uint8),
                  label: int64
               }
            )

è¯»å–æ•°æ®åº“
=================

Ray Data ä» MySQLã€PostgreSQL å’Œ MongoDB ç­‰æ•°æ®åº“è¯»å–ã€‚

.. _reading_sql:

è¯»å– SQL æ•°æ®åº“
~~~~~~~~~~~~~~~~~~~~~

è°ƒç”¨ `Python DB API2 æ ‡å‡† <https://peps.python.org/pep-0249/>`_ è¿æ¥å™¨çš„ :func:`~ray.data.read_sql` ä»æ•°æ®åº“ä¸­è¯»å–æ•°æ® ã€‚

.. tab-set::

    .. tab-item:: MySQL

        è¦ä» MySQL è¯»å–æ•°æ®ï¼Œè¯·å®‰è£…
        `MySQL Connector/Python <https://dev.mysql.com/doc/connector-python/en/>`_ã€‚å®ƒæ˜¯ç¬¬ä¸€æ–¹ MySQL æ•°æ®åº“è¿æ¥å™¨ã€‚

        .. code-block:: console

            pip install mysql-connector-python

        ç„¶åï¼Œå®šä¹‰è¿æ¥é€»è¾‘å¹¶æŸ¥è¯¢æ•°æ®åº“ã€‚

        .. testcode::
            :skipif: True

            import mysql.connector

            import ray

            def create_connection():
                return mysql.connector.connect(
                    user="admin",
                    password=...,
                    host="example-mysql-database.c2c2k1yfll7o.us-west-2.rds.amazonaws.com",
                    connection_timeout=30,
                    database="example",
                )

            # Get all movies
            dataset = ray.data.read_sql("SELECT * FROM movie", create_connection)
            # Get movies after the year 1980
            dataset = ray.data.read_sql(
                "SELECT title, score FROM movie WHERE year >= 1980", create_connection
            )
            # Get the number of movies per year
            dataset = ray.data.read_sql(
                "SELECT year, COUNT(*) FROM movie GROUP BY year", create_connection
            )


    .. tab-item:: PostgreSQL

        To read from PostgreSQL, install `Psycopg 2 <https://www.psycopg.org/docs>`_. It's
        the most popular PostgreSQL database connector.

        .. code-block:: console

            pip install psycopg2-binary

        Then, define your connection logic and query the database.

        .. testcode::
            :skipif: True

            import psycopg2

            import ray

            def create_connection():
                return psycopg2.connect(
                    user="postgres",
                    password=...,
                    host="example-postgres-database.c2c2k1yfll7o.us-west-2.rds.amazonaws.com",
                    dbname="example",
                )

            # Get all movies
            dataset = ray.data.read_sql("SELECT * FROM movie", create_connection)
            # Get movies after the year 1980
            dataset = ray.data.read_sql(
                "SELECT title, score FROM movie WHERE year >= 1980", create_connection
            )
            # Get the number of movies per year
            dataset = ray.data.read_sql(
                "SELECT year, COUNT(*) FROM movie GROUP BY year", create_connection
            )

    .. tab-item:: Snowflake

        To read from Snowflake, install the
        `Snowflake Connector for Python <https://docs.snowflake.com/en/user-guide/python-connector>`_.

        .. code-block:: console

            pip install snowflake-connector-python

        Then, define your connection logic and query the database.

        .. testcode::
            :skipif: True

            import snowflake.connector

            import ray

            def create_connection():
                return snowflake.connector.connect(
                    user=...,
                    password=...
                    account="ZZKXUVH-IPB52023",
                    database="example",
                )

            # Get all movies
            dataset = ray.data.read_sql("SELECT * FROM movie", create_connection)
            # Get movies after the year 1980
            dataset = ray.data.read_sql(
                "SELECT title, score FROM movie WHERE year >= 1980", create_connection
            )
            # Get the number of movies per year
            dataset = ray.data.read_sql(
                "SELECT year, COUNT(*) FROM movie GROUP BY year", create_connection
            )


    .. tab-item:: Databricks

        To read from Databricks, install the
        `Databricks SQL Connector for Python <https://docs.databricks.com/dev-tools/python-sql-connector.html>`_.

        .. code-block:: console

            pip install databricks-sql-connector


        Then, define your connection logic and read from the Databricks SQL warehouse.

        .. testcode::
            :skipif: True

            from databricks import sql

            import ray

            def create_connection():
                return sql.connect(
                    server_hostname="dbc-1016e3a4-d292.cloud.databricks.com",
                    http_path="/sql/1.0/warehouses/a918da1fc0b7fed0",
                    access_token=...,


            # Get all movies
            dataset = ray.data.read_sql("SELECT * FROM movie", create_connection)
            # Get movies after the year 1980
            dataset = ray.data.read_sql(
                "SELECT title, score FROM movie WHERE year >= 1980", create_connection
            )
            # Get the number of movies per year
            dataset = ray.data.read_sql(
                "SELECT year, COUNT(*) FROM movie GROUP BY year", create_connection
            )

    .. tab-item:: BigQuery

        To read from BigQuery, install the
        `Python Client for Google BigQuery <https://cloud.google.com/python/docs/reference/bigquery/latest>`_.
        This package includes a DB API2-compliant database connector.

        .. code-block:: console

            pip install google-cloud-bigquery

        Then, define your connection logic and query the dataset.

        .. testcode::
            :skipif: True

            from google.cloud import bigquery
            from google.cloud.bigquery import dbapi

            import ray

            def create_connection():
                client = bigquery.Client(...)
                return dbapi.Connection(client)

            # Get all movies
            dataset = ray.data.read_sql("SELECT * FROM movie", create_connection)
            # Get movies after the year 1980
            dataset = ray.data.read_sql(
                "SELECT title, score FROM movie WHERE year >= 1980", create_connection
            )
            # Get the number of movies per year
            dataset = ray.data.read_sql(
                "SELECT year, COUNT(*) FROM movie GROUP BY year", create_connection
            )

.. _reading_mongodb:

è¯»å– MongoDB
~~~~~~~~~~~~~~~

è¦ä» MongoDB è¯»å–æ•°æ®ï¼Œè¯·è°ƒç”¨ :func:`~ray.data.read_mongo` å¹¶æŒ‡å®šæº URIã€æ•°æ®åº“å’Œé›†åˆã€‚
æ‚¨è¿˜éœ€è¦æŒ‡å®šé’ˆå¯¹é›†åˆè¿è¡Œçš„ç®¡é“ã€‚

.. testcode::
    :skipif: True

    import ray

    # Read a local MongoDB.
    ds = ray.data.read_mongo(
        uri="mongodb://localhost:27017",
        database="my_db",
        collection="my_collection",
        pipeline=[{"$match": {"col": {"$gte": 0, "$lt": 10}}}, {"$sort": "sort_col"}],
    )

    # Reading a remote MongoDB is the same.
    ds = ray.data.read_mongo(
        uri="mongodb://username:password@mongodb0.example.com:27017/?authSource=admin",
        database="my_db",
        collection="my_collection",
        pipeline=[{"$match": {"col": {"$gte": 0, "$lt": 10}}}, {"$sort": "sort_col"}],
    )

    # Write back to MongoDB.
    ds.write_mongo(
        MongoDatasource(),
        uri="mongodb://username:password@mongodb0.example.com:27017/?authSource=admin",
        database="my_db",
        collection="my_collection",
    )

åˆ›å»ºåˆæˆæ•°æ®
=======================

ç»¼åˆæ•°æ®é›†å¯ç”¨äºæµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•ã€‚

.. tab-set::

    .. tab-item:: Int Range

        è¦ä»ä¸€ç³»åˆ—æ•´æ•°åˆ›å»ºåˆæˆ :class:`~ray.data.Dataset` ï¼Œè°ƒç”¨
        :func:`~ray.data.range`ã€‚ Ray Data å°†æ•´æ•°èŒƒå›´å­˜å‚¨åœ¨å•åˆ—ä¸­ã€‚

        .. testcode::

            import ray

            ds = ray.data.range(10000)

            print(ds.schema())

        .. testoutput::

            Column  Type
            ------  ----
            id      int64

    .. tab-item:: Tensor Range

        To create a synthetic :class:`~ray.data.Dataset` containing arrays, call
        :func:`~ray.data.range_tensor`. Ray Data packs an integer range into ndarrays of
        the provided shape.

        .. testcode::

            import ray

            ds = ray.data.range_tensor(10, shape=(64, 64))

            print(ds.schema())

        .. testoutput::

            Column  Type
            ------  ----
            data    numpy.ndarray(shape=(64, 64), dtype=int64)

åŠ è½½å…¶ä»–æ•°æ®æº
==========================

å¦‚æœ Ray Data æ— æ³•åŠ è½½æ‚¨çš„æ•°æ®ï¼Œè¯·ä½¿ç”¨
:class:`~ray.data.datasource.Datasource`ã€‚ç„¶åï¼Œæ„å»ºè‡ªå®šä¹‰æ•°æ®æºçš„å®ä¾‹å¹¶å°†å…¶ä¼ é€’ç»™
ç»™ :func:`~ray.data.read_datasource`ã€‚

.. testcode::
    :skipif: True

    # Read from a custom datasource.
    ds = ray.data.read_datasource(YourCustomDatasource(), **read_args)

    # Write to a custom datasource.
    ds.write_datasource(YourCustomDatasource(), **write_args)

æœ‰å…³ç¤ºä¾‹ï¼Œè¯·å‚é˜… :ref:`å®ç°è‡ªå®šä¹‰æ•°æ®æº <custom_datasources>`ã€‚

æ€§èƒ½è€ƒè™‘
==========================

``parallelism`` æ•°æ®é›†å†³å®šäº†åŸºç¡€æ•°æ®è¢«åˆ†å‰²æˆå¹¶è¡Œè¯»å–çš„å—æ•°ã€‚
 Ray Data åœ¨å†…éƒ¨å†³å®šåŒæ—¶è¿è¡Œå¤šå°‘ä¸ªè¯»å–ä»»åŠ¡ï¼Œä»¥å……åˆ†åˆ©ç”¨é›†ç¾¤ï¼ŒèŒƒå›´ä» ``1...parallelism`` ä¸ªè¯»å–ä»»åŠ¡ã€‚
æ¢å¥è¯è¯´ï¼Œå¹¶è¡Œåº¦è¶Šé«˜ï¼ŒDataset ä¸­çš„æ•°æ®å—è¶Šå°ï¼Œå› æ­¤å¹¶è¡Œæ‰§è¡Œçš„æœºä¼šå°±è¶Šå¤šã€‚

.. image:: images/dataset-read.svg
   :width: 650px
   :align: center

å¯ä»¥é€šè¿‡ ``parallelism`` æ•°è¦†ç›–æ­¤é»˜è®¤å¹¶è¡Œæ€§ï¼› æœ‰å…³å¦‚ä½•è°ƒæ•´è¯»å–å¹¶è¡Œæ€§çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…
:ref:`æ€§èƒ½æŒ‡å— <data_performance_tips>` ã€‚
